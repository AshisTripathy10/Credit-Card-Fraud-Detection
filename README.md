Introduction: In this project, we try to recognize fraudulent credit card transactions. The dataset, collected and analyzed during a research collaboration of Worldline and the Machine Learning Group of ULB (Université Libre de Bruxelles), contains transactions made by credit cards in September 2013 by European cardholders. We have 492 frauds out of 284,807 transactions making the dataset highly unbalanced, with the positive class (frauds) accounting for 0.172% of all transactions. Most input features have been transformed with Principal Component Analysis, the exceptions being 'Time' and 'Amount'. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. 

Importing the Modules: We import the Pandas, Matplotlib’s Pyplot and Seaborn modules for the project

Loading the Data: We load the data to our work session using Pandas read_csv() and display the top 10 rows using head()

Exploring the Data: We get the statistical descriptions for each of the numerical columns in the data using describe(). We also check if there are any null values in our data using isnull()

Exploring the Class Column: We divide the data into features (all input columns stored in X) and labels (‘Class’ column stored in y). We then calculate the percentage of the fraud transaction and valid transactions in the dataset and graphically represent the same. We observe a very high class-imbalance with more non-fraud transactions (99.83%) compared to fraudulent transactions (0.17%.) We visualize the class-imbalance using Seaborn’s countplot.

Understanding the Class Imbalance: Using this imbalanced data is not a good idea for training a model because the algorithm does not have a decent amount of fraudulent-data to learn the patterns of fraudulent transactions. Thus, it naively assumes that every transaction is non-fraudulent.

Balancing the Dataset: To make the dataset balanced, we could either undersample or oversample it. In undersampling, we remove samples of the majority class from the dataset at the cost of losing information. A simple oversampling approach involves duplicating examples in the minority class, although these examples don’t add any new information to the model. However, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE.

Preprocessing the data: We separate the original data frame into train and test sets using the train_test_split from sklearn.model_selection. We use StandardScaler to scale the "Amount" column and the "Time" column (other input features had already been scaled). We do these pre-processing steps before under/over-sampling the data because many sampling techniques require a simple model to be trained (e.g. SMOTE uses a k-NN algorithm to generate samples). These models have better performance on pre-processed datasets (e.g. both k-NN and k-means use euclidean distance, which requires the data to be normalized).

Using SMOTE: Using the distances between the closest neighbors of the minority class, SMOTE creates synthetic points in between these distances in order to reach an equal balance between the minority and majority class. We use the fit_sample method of imblearn.over_sampling.SMOTE to resample the dataset. We observe synthetic samples added in the training data with ‘Class’ value 1 (signifying fraudulent transactions).

Building the Model: We use logistic regression for this classification problem. We declare some parameters and their values for the grid-search for hyperparameter tuning. Parameter C is the regularization parameter, and penalty is the norm used in the penalization. We print the best parameters for the logistic regression model.

Evaluating the Model: We use the confusion_matrix function imported from sklearn.metrics on the resampled training and testing data to calculate the recall values. For binary classification in confusion matrix, the count of true negatives is C00, false negatives is C10, true positives is C11 and false positives is C01. Recall is the ratio between positive (fraudulent transactions) cases predicted by the model and the total number of positive cases that exist (C11/C11+C10). We are optimizing for recall because it is tolerable to classify some valid transactions as fraudulent, but it is not tolerable to misclassify the fraudulent transactions as valid ones. The recall value for both train and test sets comes around 0.918 (91.80%)

Visualizing the Confusion Matrix and ROC-AUC Curve: We use the plot_confusion_matrix from sklearn.metrics to visualize the confusion matrices for the predictions made on the test set and over-sampled train set. We also plot the ROC-AUC curve. The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the True Positive Rate against False Positive Rate at various threshold values. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes. We use the roc_curve function and the roc_auc function to finally get the AUC value of 0.983. We visualize the ROC-AUC curve using plt.plot

